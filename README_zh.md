<small>[EN](README.md) | ç®€ä½“ä¸­æ–‡ </small>

# [AnglEğŸ“: Angle-optimized Text Embeddings](https://arxiv.org/abs/2309.12871)

> It is Angle ğŸ“, not Angel ğŸ‘¼.

ğŸ”¥ åŸºäº AnglE å¼€ç®±å³ç”¨çš„æ–‡æœ¬å‘é‡åº“ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œå¯ç”¨äºæ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ã€æ£€ç´¢å¬å›ã€åŒ¹é…ç­‰åœºæ™¯ã€‚ä»£ç åŸºäº ğŸ¤—transformers æ„å»ºï¼Œæä¾›æ˜“ç”¨çš„å¾®è°ƒæ¥å£ï¼Œå¯åœ¨ 3090Tiã€ 4090 ç­‰æ¶ˆè´¹çº§ GPU ä¸Šå¾®è°ƒ LLaMA-7B æ¨¡å‹ï¼Œæ”¯æŒå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒã€‚


<a href="https://arxiv.org/abs/2309.12871">
    <img src="https://img.shields.io/badge/Arxiv-2306.06843-yellow.svg?style=flat-square" alt="https://arxiv.org/abs/2309.12871" />
</a>
<a href="https://pypi.org/project/angle_emb/">
    <img src="https://img.shields.io/pypi/v/angle_emb?style=flat-square" alt="PyPI version" />
</a>
<a href="https://pypi.org/project/angle_emb/">
    <img src="https://img.shields.io/pypi/dm/angle_emb?style=flat-square" alt="PyPI Downloads" />
</a>
<a href="http://makeapullrequest.com">
    <img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="http://makeapullrequest.com" />
</a>

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/angle-optimized-text-embeddings/semantic-textual-similarity-on-sick-r-1)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sick-r-1?p=angle-optimized-text-embeddings)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/angle-optimized-text-embeddings/semantic-textual-similarity-on-sts16)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts16?p=angle-optimized-text-embeddings)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/angle-optimized-text-embeddings/semantic-textual-similarity-on-sts15)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts15?p=angle-optimized-text-embeddings)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/angle-optimized-text-embeddings/semantic-textual-similarity-on-sts14)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts14?p=angle-optimized-text-embeddings)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/angle-optimized-text-embeddings/semantic-textual-similarity-on-sts13)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts13?p=angle-optimized-text-embeddings)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/angle-optimized-text-embeddings/semantic-textual-similarity-on-sts12)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts12?p=angle-optimized-text-embeddings)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/angle-optimized-text-embeddings/semantic-textual-similarity-on-sts-benchmark)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark?p=angle-optimized-text-embeddings)


## æ¨¡å‹åˆ—è¡¨

| ğŸ¤— HF | Backbone | LLM | Language | Prompt | Datasets | Pooling Strategy |
|----|------|------|------|------|------|------|
| [SeanLee97/angle-llama-7b-nli-v2](https://huggingface.co/SeanLee97/angle-llama-7b-nli-v2) |  NousResearch/Llama-2-7b-hf | Y | EN | `Prompts.A` | multi_nli + snli | last token |
| [SeanLee97/angle-llama-7b-nli-20231027](https://huggingface.co/SeanLee97/angle-llama-7b-nli-20231027) |  NousResearch/Llama-2-7b-hf | Y | EN | `Prompts.A` | multi_nli + snli | last token |
| [SeanLee97/angle-bert-base-uncased-nli-en-v1](https://huggingface.co/SeanLee97/angle-bert-base-uncased-nli-en-v1) |  bert-base-uncased | N | EN | N | multi_nli + snli | `cls_avg` |
| [SeanLee97/angle-roberta-wwm-base-zhnli-v1](https://huggingface.co/SeanLee97/angle-roberta-wwm-base-zhnli-v1) |  hfl/chinese-roberta-wwm-ext | N | ZH-CN | N | zh_nli_all | `cls` |
| [SeanLee97/angle-llama-7b-zhnli-v1](https://huggingface.co/SeanLee97/angle-llama-7b-zhnli-v1) |  NousResearch/Llama-2-7b-hf | Y | ZH-CN | `Prompts.B` | zh_nli_all | last token |


## æ¨¡å‹æ•ˆæœ

> ä¸­æ–‡æ–‡æ¡£åªåˆ—å‡ºä¸­æ–‡æ¨¡å‹çš„ç»“æœï¼Œè‹±æ–‡è¯·å‚ç…§ä¸»æ–‡æ¡£

### è¿ç§» STS

ä½¿ç”¨ AnglE åœ¨ [shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all) ä¸Šè®­ç»ƒ (åŠ å…¥äº†éƒ¨åˆ† [shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) train set çš„æ•°æ®)ï¼Œå¹¶åœ¨ä»¥ä¸‹ 7 ä¸ªæ•°æ®é›†çš„ test set ä¸Šè¯„æµ‹ã€‚


| æ¨¡å‹ | ATEC | BQ	| LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc | Avg. |
| ------- |-------|-------|-------|-------|-------|--------------|-----------------|-------|
| ^[shibing624/text2vec-bge-large-chinese](https://huggingface.co/shibing624/text2vec-bge-large-chinese) | 38.41 | 61.34 | 71.72 | 35.15 | 76.44 | 71.81 | 63.15 | 59.72 |
| ^[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase) |	44.89 | 63.58 | 74.24 | 40.90 | 78.93 | 76.70 | 63.30 | 63.08 |
| [SeanLee97/angle-roberta-wwm-base-zhnli-v1](https://huggingface.co/SeanLee97/angle-roberta-wwm-base-zhnli-v1) | 49.49 | 72.47 | 78.33 | 59.13 | 77.14 |    72.36     |      60.53      | **67.06** |
| [SeanLee97/angle-llama-7b-zhnli-v1](https://huggingface.co/SeanLee97/angle-llama-7b-zhnli-v1) | 50.44 | 71.95 | 78.90 | 56.57 | 81.11 | 68.11 | 52.02 | 65.59 |

^ è¡¨ç¤º baselineï¼Œç»“æœæ¥è‡ªï¼šhttps://github.com/shibing624/text2vec

AnglE çš„è¯„æµ‹å¯å‚ç…§ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1OvvBu4c3pfkf_XcVuFyCqsAqCUArtZZ8#scrollTo=xFjso-JLNsKZ)

> å‘ç°ï¼šLLaMA2-7B çš„ä¸­æ–‡è¡¨è¾¾èƒ½åŠ›è¿˜æœ‰å¾…æå‡ã€‚
 

### éè¿ç§» STS

åœ¨æ•°æ®é›†å„è‡ªçš„ train set ä¸Šè®­ç»ƒï¼Œå„è‡ªçš„ test set ä¸Šè¯„æµ‹ï¼Œæ•ˆæœå¦‚ä¸‹è¡¨ï¼š

| æ¨¡å‹ | ATEC | BQ	| LCQMC | PAWSX | STS-B | Avg. |
| ------- |-------|-------|-------|-------|-------|--------------|
| ^CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |
| ^CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |
| ^CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
| AnglE-roberta-wwm-ext | **51.30** | **73.35** | **79.53** | **62.46** | **80.83** | **69.49** |

^ ä¸º baseline æ¨¡å‹ï¼Œç»“æœæ¥è‡ªï¼š[text2vec](https://github.com/shibing624/text2vec#%E4%B8%AD%E6%96%87%E5%8C%B9%E9%85%8D%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%AF%84%E6%B5%8B%E7%BB%93%E6%9E%9C)


AnglE-roberta-wwm-ext å„æ•°æ®é›†çš„å¾®è°ƒåŠè¯„ä¼°ä»£ç å¦‚ä¸‹ï¼š
- ATEC: [examples/Angle-ATEC.ipynb](examples/Angle-ATEC.ipynb)
- BQ: [examples/Angle-BQ.ipynb](examples/Angle-BQ.ipynb)
- LCQMC: [examples/Angle-LCQMC.ipynb](examples/Angle-LCQMC.ipynb)
- PAWSX: [examples/Angle-PAWSX.ipynb](examples/Angle-PAWSX.ipynb)
- SST-B: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1HzuaZjdkKqL_JasQnSGZ3g2H3H2aR6yG?usp=sharing)


## ç”¨æ³•

AnglE æ”¯æŒä¸¤ç§è°ƒç”¨æ–¹å¼ï¼Œä¸€ç§æ˜¯ AnglE æä¾›çš„åº“ï¼Œå¦ä¸€ç§æ˜¯ä½¿ç”¨ huggingface transformersã€‚æ¨èä½¿ç”¨ AnglE æä¾›çš„åº“ï¼Œå¯ä»¥çœå»è¾ƒå¤šè°ƒç”¨æ­¥éª¤ã€‚è¦ä½¿ç”¨ AnglE åº“éœ€è¦å…ˆå®‰è£… `angle_emb`ï¼Œå¦‚ä¸‹ï¼š

```bash
python -m pip install -U angle-emb
```

### Angle-LLaMA

1) AnglE
```python
from angle_emb import AnglE, Prompts

angle = AnglE.from_pretrained('NousResearch/Llama-2-7b-hf', pretrained_lora_path='SeanLee97/angle-llama-7b-zhnli-v1')
# è¯·é€‰æ‹©å¯¹åº”çš„ promptï¼Œæ­¤æ¨¡å‹å¯¹åº” Prompts.B
print('All predefined prompts:', Prompts.list_prompts())
angle.set_prompt(prompt=Prompts.B)
print('prompt:', angle.prompt)
vec = angle.encode({'text': 'ä½ å¥½ä¸–ç•Œ'}, to_numpy=True)
print(vec)
vecs = angle.encode([{'text': 'ä½ å¥½ä¸–ç•Œ1'}, {'text': 'ä½ å¥½ä¸–ç•Œ2'}], to_numpy=True)
print(vecs)
```

2) transformers

```python
from angle_emb import Prompts
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

peft_model_id = 'SeanLee97/angle-llama-7b-zhnli-v1'
config = PeftConfig.from_pretrained(peft_model_id)
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path).bfloat16().cuda()
model = PeftModel.from_pretrained(model, peft_model_id).cuda()

def decorate_text(text: str):
    return Prompts.B.format(text=text)

inputs = 'ä½ å¥½ä¸–ç•Œ'
tok = tokenizer([decorate_text(inputs)], return_tensors='pt')
for k, v in tok.items():
    tok[k] = v.cuda()
vec = model(output_hidden_states=True, **tok).hidden_states[-1][:, -1].float().detach().cpu().numpy()
print(vec)
```

### Angle-BERT

1) AnglE
```python
from angle_emb import AnglE

angle = AnglE.from_pretrained('SeanLee97/angle-roberta-wwm-base-zhnli-v1', pooling_strategy='cls').cuda()
vec = angle.encode('ä½ å¥½ä¸–ç•Œ', to_numpy=True)
print(vec)
vecs = angle.encode(['ä½ å¥½ä¸–ç•Œ1', 'ä½ å¥½ä¸–ç•Œ2'], to_numpy=True)
print(vecs)
```

2) transformers

```python
import torch
from transformers import AutoModel, AutoTokenizer

model_id = 'SeanLee97/angle-roberta-wwm-base-zhnli-v1'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModel.from_pretrained(model_id).cuda()

inputs = 'ä½ å¥½ä¸–ç•Œ'
tok = tokenizer([inputs], return_tensors='pt')
for k, v in tok.items():
    tok[k] = v.cuda()
hidden_state = model(**tok).last_hidden_state
vec = hidden_state[:, 0]
print(vec)
```

## å¾®è°ƒæ¨¡å‹

åªéœ€è¦å‡†å¤‡å¥½æ•°æ®å³å¯å¿«é€Ÿå¾®è°ƒã€‚æ•°æ®æ ¼å¼å¿…é¡»è¦è½¬æˆ `datasets.Dataset` ï¼ˆä½¿ç”¨æ–¹å¼è¯·å‚ç…§å®˜æ–¹æ–‡æ¡£ [Datasets](https://huggingface.co/docs/datasets/index)ï¼‰ä¸”å¿…é¡»è¦æä¾› `text1`, `text2`, `label` ä¸‰åˆ—ã€‚

ä¸‹é¢ä»¥å¾®è°ƒä¸­æ–‡ç‰ˆæœ¬ STS-B ä¸ºä¾‹ï¼Œæ¼”ç¤ºæ•´ä¸ªå¾®è°ƒè®­ç»ƒåŠè¯„ä¼°çš„è¿‡ç¨‹ã€‚

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1HzuaZjdkKqL_JasQnSGZ3g2H3H2aR6yG?usp=sharing)


```python
from datasets import load_dataset
from angle_emb import AnglE, AngleDataTokenizer

# 1. åŠ è½½æ¨¡å‹
angle = AnglE.from_pretrained('hfl/chinese-roberta-wwm-ext', max_length=128, pooling_strategy='cls').cuda()

# 2. åŠ è½½æ•°æ®å¹¶è½¬æ¢æ•°æ®
ds = load_dataset('shibing624/nli_zh', 'STS-B')
ds = ds.rename_column('sentence1', 'text1')
ds = ds.rename_column('sentence2', 'text2')
ds = ds.select_columns(["text1", "text2", "label"])
train_ds = ds['train'].shuffle().map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)
valid_ds = ds['validation'].map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)
test_ds = ds['test'].map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)

# 3. è®­ç»ƒ
angle.fit(
    train_ds=train_ds,
    valid_ds=valid_ds,
    output_dir='ckpts/sts-b',
    batch_size=64,
    epochs=5,
    learning_rate=3e-5,
    save_steps=100,
    eval_steps=1000,
    warmup_steps=0,
    gradient_accumulation_steps=1,
    loss_kwargs={
        'w1': 1.0,
        'w2': 1.0,
        'w3': 1.0,
        'cosine_tau': 20,
        'ibn_tau': 20,
        'angle_tau': 1.0
    },
    fp16=True,
    logging_steps=100
)

# 4. åŠ è½½æœ€ä¼˜æ¨¡å‹è¯„ä¼°æ•ˆæœ
angle = AnglE.from_pretrained('hfl/chinese-roberta-wwm-ext', pretrained_model_path='ckpts/sts-b/best-checkpoint').cuda()
corrcoef, accuracy = angle.evaluate(test_ds, device=angle.device)
print('corrcoef:', corrcoef)
```

# å¼•ç”¨ Citation

å¦‚æœä½ æœ‰ä½¿ç”¨æˆ‘ä»¬çš„ä»£ç åŠé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ¬¢è¿ç»™æˆ‘ä»¬ä¸‰è¿ï¼Œä¸‰è¿æ–¹å¼ä¸ºï¼š
1) ç»™æœ¬é¡¹ç›® GitHub åŠ ä¸ª star
2) ç²˜è´´ä»¥ä¸‹å¼•ç”¨ä¿¡æ¯åˆ°ä½  paper çš„ bibtex
3) åœ¨ä½ çš„ paper æ­£æ–‡ä¸­å¼•ç”¨

```bibtex
@article{li2023angle,
  title={AnglE-Optimized Text Embeddings},
  author={Li, Xianming and Li, Jing},
  journal={arXiv preprint arXiv:2309.12871},
  year={2023}
}
```

å½“ä½ ä½¿ç”¨æˆ‘ä»¬çš„ LLM æ¨¡å‹ä¸”ä½¿ç”¨çš„ prompt ä¸º `xxx in one word:` æ—¶ï¼Œå»ºè®®é¡ºå¸¦å¼•ç”¨ä»¥ä¸‹æ–‡ç« :

```bibtex
@article{jiang2023scaling,
  title={Scaling Sentence Embeddings with Large Language Models},
  author={Jiang, Ting and Huang, Shaohan and Luan, Zhongzhi and Wang, Deqing and Zhuang, Fuzhen},
  journal={arXiv preprint arXiv:2307.16645},
  year={2023}
}
```


# FAQ

1) ä¸ºä»€ä¹ˆä¸ä½¿ç”¨å›½äº§ LLMï¼Ÿ

å„ä¸ªéƒ½é¥é¥é¢†å…ˆï¼Œé€‰æ‹©å›°éš¾ç—‡ï¼Œæ ‡å‡†å¤ªå¤šï¼Œæ²¡æ—¶é—´æï¼Œå¤§å®¶å¯ä»¥è‡ªè¡Œå°è¯•ã€‚


2) æ¨¡å‹å¯å•†ç”¨å˜›ï¼Ÿ

åœ¨æ²¡æœ‰ç‰¹åˆ«å£°æ˜ä¸‹ï¼Œå¯å•†ç”¨ï¼Œä½†è¯·éµå¾ª backbone æ¨¡å‹æœ¬èº«çš„åè®®ã€‚


3) ä»£ç è·Ÿæˆ‘çš„ç¯å¢ƒå†²çªå’‹åŠï¼Ÿ

æœ¬ä»£ç åŸºäº transformers æ„å»ºï¼Œtransformers èƒ½è·‘é€šï¼Œæœ¬ä»£ç åº”è¯¥ä¹Ÿèƒ½è·‘é€šï¼Œå¦‚æœå‡ºé—®é¢˜è¯·å°è¯•æŒ‰ç…§ transformers çš„æ–¹å‘è§£å†³ã€‚
æ—¶é—´å› ç´ ï¼Œåªèƒ½ç”©é”… transformersï¼Œè¯·æµ·æ¶µã€‚


4) é¢†åŸŸæ•ˆæœä¸ä½³å’‹åŠï¼Ÿ

é¢†åŸŸæ•ˆæœä¸ä½³è¯·è‡ªå·±æ ‡æ•°æ®å¾®è°ƒè§£å†³ï¼Œæ—¶é—´å®è´µï¼Œæˆ‘ä»¬ä¸åšå…è´¹å¤–åŒ…ã€‚å’¨è¯¢æ”¶è´¹ï¼Œå®šåˆ¶æ”¶è´¹ ï¼ˆæœ‰å¿å’¨è¯¢é€šé“ğŸ“® xmlee97@gmail.comï¼‰ã€‚
